"""
LSTMçœ æ°—æ¨å®šãƒ¢ãƒ‡ãƒ«
LSTM-based Drowsiness Estimation Model

è«–æ–‡ã§ææ¡ˆã•ã‚ŒãŸLSTMã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãçœ æ°—æ¨å®šãƒ¢ãƒ‡ãƒ«
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import Dict, List, Tuple, Optional
import json
import os
from datetime import datetime


class DrowsinessLSTM(nn.Module):
    """
    çœ æ°—æ¨å®šç”¨LSTMãƒ¢ãƒ‡ãƒ«
    
    ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:
    - å…¥åŠ›: (batch_size, sequence_length=10, features=6)
    - LSTMå±¤1: 64ãƒ¦ãƒ‹ãƒƒãƒˆ + Dropout(0.3)
    - LSTMå±¤2: 32ãƒ¦ãƒ‹ãƒƒãƒˆ + Dropout(0.3)
    - å…¨çµåˆå±¤: 32ãƒ¦ãƒ‹ãƒƒãƒˆ + ReLU
    - å‡ºåŠ›å±¤: 2ã‚¯ãƒ©ã‚¹ (æ­£å¸¸/çœ æ°—) + Softmax
    """
    
    def __init__(self, input_size=6, hidden_size1=64, hidden_size2=32, 
                 fc_size=32, num_classes=2, dropout_rate=0.3):
        """
        åˆæœŸåŒ–
        
        Args:
            input_size (int): å…¥åŠ›ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 6ï¼‰
            hidden_size1 (int): LSTMç¬¬1å±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
            hidden_size2 (int): LSTMç¬¬2å±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
            fc_size (int): å…¨çµåˆå±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
            num_classes (int): å‡ºåŠ›ã‚¯ãƒ©ã‚¹æ•°ï¼ˆæ­£å¸¸/çœ æ°— = 2ï¼‰
            dropout_rate (float): ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(DrowsinessLSTM, self).__init__()
        
        self.input_size = input_size
        self.hidden_size1 = hidden_size1
        self.hidden_size2 = hidden_size2
        self.fc_size = fc_size
        self.num_classes = num_classes
        self.dropout_rate = dropout_rate
        
        # LSTMå±¤1
        self.lstm1 = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size1,
            num_layers=1,
            batch_first=True,
            dropout=0  # å˜å±¤ãªã®ã§ã“ã“ã§ã¯0
        )
        self.dropout1 = nn.Dropout(dropout_rate)
        
        # LSTMå±¤2
        self.lstm2 = nn.LSTM(
            input_size=hidden_size1,
            hidden_size=hidden_size2,
            num_layers=1,
            batch_first=True,
            dropout=0
        )
        self.dropout2 = nn.Dropout(dropout_rate)
        
        # å…¨çµåˆå±¤
        self.fc1 = nn.Linear(hidden_size2, fc_size)
        self.relu = nn.ReLU()
        self.dropout3 = nn.Dropout(dropout_rate)
        
        # å‡ºåŠ›å±¤
        self.fc2 = nn.Linear(fc_size, num_classes)
        
        # åˆæœŸåŒ–
        self._init_weights()
    
    def _init_weights(self):
        """é‡ã¿ã®åˆæœŸåŒ–"""
        for name, param in self.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                param.data.fill_(0)
            elif 'fc' in name and 'weight' in name:
                nn.init.xavier_uniform_(param.data)
    
    def forward(self, x):
        """
        é †ä¼æ’­
        
        Args:
            x (torch.Tensor): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (batch_size, sequence_length, input_size)
        
        Returns:
            torch.Tensor: å‡ºåŠ› (batch_size, num_classes)
        """
        # LSTMå±¤1
        out, (h1, c1) = self.lstm1(x)
        out = self.dropout1(out)
        
        # LSTMå±¤2
        out, (h2, c2) = self.lstm2(out)
        out = self.dropout2(out)
        
        # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‚’å–å¾—
        out = out[:, -1, :]
        
        # å…¨çµåˆå±¤
        out = self.fc1(out)
        out = self.relu(out)
        out = self.dropout3(out)
        
        # å‡ºåŠ›å±¤
        out = self.fc2(out)
        
        return out
    
    def predict_proba(self, x):
        """
        ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’äºˆæ¸¬
        
        Args:
            x (torch.Tensor): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            torch.Tensor: ã‚¯ãƒ©ã‚¹ç¢ºç‡ (batch_size, num_classes)
        """
        self.eval()
        with torch.no_grad():
            logits = self.forward(x)
            probs = torch.softmax(logits, dim=1)
        return probs
    
    def predict(self, x):
        """
        ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬
        
        Args:
            x (torch.Tensor): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            torch.Tensor: äºˆæ¸¬ã‚¯ãƒ©ã‚¹ (batch_size,)
        """
        probs = self.predict_proba(x)
        return torch.argmax(probs, dim=1)


class BlinkSequenceDataset(Dataset):
    """
    ç¬ãã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    """
    
    def __init__(self, sequences, labels):
        """
        åˆæœŸåŒ–
        
        Args:
            sequences (np.ndarray): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ (n_samples, sequence_length, features)
            labels (np.ndarray): ãƒ©ãƒ™ãƒ« (n_samples,) 0: æ­£å¸¸, 1: çœ æ°—
        """
        self.sequences = torch.FloatTensor(sequences)
        self.labels = torch.LongTensor(labels)
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]


class DrowsinessEstimator:
    """
    çœ æ°—æ¨å®šã‚·ã‚¹ãƒ†ãƒ ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãƒ»æ¨è«–å™¨
    """
    
    def __init__(self, model_params=None, device=None):
        """
        åˆæœŸåŒ–
        
        Args:
            model_params (dict): ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            device (str): ãƒ‡ãƒã‚¤ã‚¹ ('cuda' or 'cpu')
        """
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        default_params = {
            'input_size': 6,
            'hidden_size1': 64,
            'hidden_size2': 32,
            'fc_size': 32,
            'num_classes': 2,
            'dropout_rate': 0.3
        }
        
        if model_params is not None:
            default_params.update(model_params)
        
        self.model_params = default_params
        self.model = DrowsinessLSTM(**default_params).to(self.device)
        
        # è¨“ç·´å±¥æ­´
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        # Early Stoppingç”¨
        self.best_val_loss = float('inf')
        self.best_model_state = None
        self.patience_counter = 0
        
        print(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
        print(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self._count_parameters():,}")
    
    def _count_parameters(self):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)
    
    def train_model(self, train_sequences, train_labels, 
                   val_sequences=None, val_labels=None,
                   epochs=100, batch_size=32, learning_rate=0.001,
                   patience=10, verbose=True):
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
        
        Args:
            train_sequences (np.ndarray): è¨“ç·´ç”¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
            train_labels (np.ndarray): è¨“ç·´ç”¨ãƒ©ãƒ™ãƒ«
            val_sequences (np.ndarray): æ¤œè¨¼ç”¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
            val_labels (np.ndarray): æ¤œè¨¼ç”¨ãƒ©ãƒ™ãƒ«
            epochs (int): ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate (float): å­¦ç¿’ç‡
            patience (int): Early Stoppingã®å¿è€å€¤
            verbose (bool): è©³ç´°è¡¨ç¤º
        
        Returns:
            dict: è¨“ç·´å±¥æ­´
        """
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_dataset = BlinkSequenceDataset(train_sequences, train_labels)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        if val_sequences is not None and val_labels is not None:
            val_dataset = BlinkSequenceDataset(val_sequences, val_labels)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
            use_validation = True
        else:
            use_validation = False
        
        # æå¤±é–¢æ•°ã¨æœ€é©åŒ–æ‰‹æ³•
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        
        print(f"\nğŸš€ è¨“ç·´é–‹å§‹")
        print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {epochs}")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        print(f"   å­¦ç¿’ç‡: {learning_rate}")
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_sequences)}")
        if use_validation:
            print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ•°: {len(val_sequences)}")
        print("=" * 70)
        
        # è¨“ç·´ãƒ«ãƒ¼ãƒ—
        for epoch in range(epochs):
            # è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_sequences, batch_labels in train_loader:
                batch_sequences = batch_sequences.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                # é †ä¼æ’­
                optimizer.zero_grad()
                outputs = self.model(batch_sequences)
                loss = criterion(outputs, batch_labels)
                
                # é€†ä¼æ’­
                loss.backward()
                optimizer.step()
                
                # çµ±è¨ˆ
                train_loss += loss.item() * batch_sequences.size(0)
                _, predicted = torch.max(outputs.data, 1)
                train_total += batch_labels.size(0)
                train_correct += (predicted == batch_labels).sum().item()
            
            # ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®å¹³å‡
            epoch_train_loss = train_loss / train_total
            epoch_train_acc = 100.0 * train_correct / train_total
            
            self.history['train_loss'].append(epoch_train_loss)
            self.history['train_acc'].append(epoch_train_acc)
            
            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
            if use_validation:
                val_loss, val_acc = self._validate(val_loader, criterion)
                self.history['val_loss'].append(val_loss)
                self.history['val_acc'].append(val_acc)
                
                # Early Stopping
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.best_model_state = self.model.state_dict().copy()
                    self.patience_counter = 0
                else:
                    self.patience_counter += 1
                
                if verbose and (epoch + 1) % 5 == 0:
                    print(f"Epoch [{epoch+1:3d}/{epochs}] "
                          f"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.2f}% | "
                          f"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%")
                
                # Early Stoppingãƒã‚§ãƒƒã‚¯
                if self.patience_counter >= patience:
                    print(f"\nâ¹ï¸  Early Stopping at epoch {epoch+1}")
                    print(f"   Best validation loss: {self.best_val_loss:.4f}")
                    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å¾©å…ƒ
                    self.model.load_state_dict(self.best_model_state)
                    break
            else:
                if verbose and (epoch + 1) % 5 == 0:
                    print(f"Epoch [{epoch+1:3d}/{epochs}] "
                          f"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.2f}%")
        
        print("=" * 70)
        print("âœ… è¨“ç·´å®Œäº†")
        
        return self.history
    
    def _validate(self, val_loader, criterion):
        """
        æ¤œè¨¼ã‚’å®Ÿè¡Œ
        
        Args:
            val_loader: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
            criterion: æå¤±é–¢æ•°
        
        Returns:
            tuple: (æ¤œè¨¼æå¤±, æ¤œè¨¼ç²¾åº¦)
        """
        self.model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch_sequences, batch_labels in val_loader:
                batch_sequences = batch_sequences.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                outputs = self.model(batch_sequences)
                loss = criterion(outputs, batch_labels)
                
                val_loss += loss.item() * batch_sequences.size(0)
                _, predicted = torch.max(outputs.data, 1)
                val_total += batch_labels.size(0)
                val_correct += (predicted == batch_labels).sum().item()
        
        epoch_val_loss = val_loss / val_total
        epoch_val_acc = 100.0 * val_correct / val_total
        
        return epoch_val_loss, epoch_val_acc
    
    def predict(self, sequences):
        """
        çœ æ°—çŠ¶æ…‹ã‚’äºˆæ¸¬
        
        Args:
            sequences (np.ndarray): å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
        
        Returns:
            np.ndarray: äºˆæ¸¬ã‚¯ãƒ©ã‚¹ (0: æ­£å¸¸, 1: çœ æ°—)
        """
        self.model.eval()
        sequences_tensor = torch.FloatTensor(sequences).to(self.device)
        
        with torch.no_grad():
            predictions = self.model.predict(sequences_tensor)
        
        return predictions.cpu().numpy()
    
    def predict_proba(self, sequences):
        """
        çœ æ°—çŠ¶æ…‹ã®ç¢ºç‡ã‚’äºˆæ¸¬
        
        Args:
            sequences (np.ndarray): å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
        
        Returns:
            np.ndarray: ã‚¯ãƒ©ã‚¹ç¢ºç‡ (n_samples, 2)
        """
        self.model.eval()
        sequences_tensor = torch.FloatTensor(sequences).to(self.device)
        
        with torch.no_grad():
            probabilities = self.model.predict_proba(sequences_tensor)
        
        return probabilities.cpu().numpy()
    
    def evaluate(self, test_sequences, test_labels):
        """
        ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
        
        Args:
            test_sequences (np.ndarray): ãƒ†ã‚¹ãƒˆç”¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
            test_labels (np.ndarray): ãƒ†ã‚¹ãƒˆç”¨ãƒ©ãƒ™ãƒ«
        
        Returns:
            dict: è©•ä¾¡çµæœ
        """
        predictions = self.predict(test_sequences)
        accuracy = 100.0 * np.mean(predictions == test_labels)
        
        # æ··åŒè¡Œåˆ—
        from sklearn.metrics import confusion_matrix, classification_report
        cm = confusion_matrix(test_labels, predictions)
        report = classification_report(test_labels, predictions, 
                                       target_names=['æ­£å¸¸', 'çœ æ°—'],
                                       output_dict=True)
        
        results = {
            'accuracy': accuracy,
            'confusion_matrix': cm.tolist(),
            'classification_report': report
        }
        
        print(f"\nğŸ“Š è©•ä¾¡çµæœ:")
        print(f"   æ­£è§£ç‡: {accuracy:.2f}%")
        print(f"\næ··åŒè¡Œåˆ—:")
        print(f"              äºˆæ¸¬: æ­£å¸¸  çœ æ°—")
        print(f"   å®Ÿéš›: æ­£å¸¸     {cm[0, 0]:5d}  {cm[0, 1]:5d}")
        print(f"         çœ æ°—     {cm[1, 0]:5d}  {cm[1, 1]:5d}")
        
        return results
    
    def save_model(self, filepath, include_history=True):
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        
        Args:
            filepath (str): ä¿å­˜å…ˆãƒ‘ã‚¹
            include_history (bool): è¨“ç·´å±¥æ­´ã‚‚ä¿å­˜ã™ã‚‹ã‹
        """
        save_dict = {
            'model_state_dict': self.model.state_dict(),
            'model_params': self.model_params,
            'device': str(self.device)
        }
        
        if include_history:
            save_dict['history'] = self.history
        
        torch.save(save_dict, filepath)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
    
    def load_model(self, filepath):
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            filepath (str): èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.model_params = checkpoint['model_params']
        self.model = DrowsinessLSTM(**self.model_params).to(self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        if 'history' in checkpoint:
            self.history = checkpoint['history']
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {filepath}")
    
    def get_model_summary(self):
        """ãƒ¢ãƒ‡ãƒ«ã®æ¦‚è¦ã‚’è¡¨ç¤º"""
        print("\n" + "=" * 70)
        print("ğŸ“‹ ãƒ¢ãƒ‡ãƒ«æ¦‚è¦")
        print("=" * 70)
        print(self.model)
        print("=" * 70)
        print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self._count_parameters():,}")
        print("=" * 70)


# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    print("=" * 70)
    print("LSTMçœ æ°—æ¨å®šãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    np.random.seed(42)
    n_samples = 200
    sequence_length = 10
    n_features = 6
    
    # æ­£å¸¸çŠ¶æ…‹ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆç¬ãä¿‚æ•°ãŒé«˜ã‚ï¼‰
    normal_sequences = np.random.randn(n_samples // 2, sequence_length, n_features).astype(np.float32)
    normal_sequences[:, :, 0] = np.abs(normal_sequences[:, :, 0]) + 1.2  # ç¬ãä¿‚æ•°
    normal_labels = np.zeros(n_samples // 2, dtype=np.int64)
    
    # çœ æ°—çŠ¶æ…‹ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆç¬ãä¿‚æ•°ãŒä½ã‚ã€æ™‚é–“ãŒé•·ã‚ï¼‰
    drowsy_sequences = np.random.randn(n_samples // 2, sequence_length, n_features).astype(np.float32)
    drowsy_sequences[:, :, 0] = np.abs(drowsy_sequences[:, :, 0]) + 0.6  # ç¬ãä¿‚æ•°ï¼ˆä½ã‚ï¼‰
    drowsy_sequences[:, :, 1:3] = np.abs(drowsy_sequences[:, :, 1:3]) + 0.5  # æ™‚é–“ï¼ˆé•·ã‚ï¼‰
    drowsy_labels = np.ones(n_samples // 2, dtype=np.int64)
    
    # ãƒ‡ãƒ¼ã‚¿ã®çµåˆã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    all_sequences = np.vstack([normal_sequences, drowsy_sequences])
    all_labels = np.hstack([normal_labels, drowsy_labels])
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    indices = np.random.permutation(n_samples)
    all_sequences = all_sequences[indices]
    all_labels = all_labels[indices]
    
    # è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆåˆ†å‰²
    train_size = int(0.7 * n_samples)
    val_size = int(0.15 * n_samples)
    
    train_sequences = all_sequences[:train_size]
    train_labels = all_labels[:train_size]
    val_sequences = all_sequences[train_size:train_size + val_size]
    val_labels = all_labels[train_size:train_size + val_size]
    test_sequences = all_sequences[train_size + val_size:]
    test_labels = all_labels[train_size + val_size:]
    
    print(f"\nğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
    print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_sequences)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_sequences)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_sequences)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    estimator = DrowsinessEstimator()
    estimator.get_model_summary()
    
    # è¨“ç·´
    print("\n" + "=" * 70)
    history = estimator.train_model(
        train_sequences, train_labels,
        val_sequences, val_labels,
        epochs=30,
        batch_size=16,
        learning_rate=0.001,
        patience=10,
        verbose=True
    )
    
    # è©•ä¾¡
    print("\n" + "=" * 70)
    results = estimator.evaluate(test_sequences, test_labels)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã®ãƒ†ã‚¹ãƒˆ
    print("\n" + "=" * 70)
    test_model_path = 'drowsiness_lstm_test.pth'
    estimator.save_model(test_model_path)
    
    # äºˆæ¸¬ã®ãƒ†ã‚¹ãƒˆ
    print("\n" + "=" * 70)
    print("ğŸ”® äºˆæ¸¬ãƒ†ã‚¹ãƒˆ:")
    sample = test_sequences[:1]
    pred_class = estimator.predict(sample)
    pred_proba = estimator.predict_proba(sample)
    
    print(f"   å…¥åŠ›å½¢çŠ¶: {sample.shape}")
    print(f"   äºˆæ¸¬ã‚¯ãƒ©ã‚¹: {pred_class[0]} ({'æ­£å¸¸' if pred_class[0] == 0 else 'çœ æ°—'})")
    print(f"   ç¢ºç‡: æ­£å¸¸={pred_proba[0, 0]:.3f}, çœ æ°—={pred_proba[0, 1]:.3f}")
    
    print("\n" + "=" * 70)
    print("ãƒ†ã‚¹ãƒˆå®Œäº† âœ…")
    print("=" * 70)
