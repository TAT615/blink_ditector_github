"""
çœ æ°—æ¨å®šç”¨ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Drowsiness Data Manager

åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã€åˆ†å‰²ã‚’è¡Œã„ã¾ã™ã€‚
"""

import os
import json
import csv
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import glob


class DrowsinessDataManager:
    """
    çœ æ°—æ¨å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç®¡ç†ã‚¯ãƒ©ã‚¹
    
    æ©Ÿèƒ½:
    - ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    - æ­£è¦åŒ–ãƒ»å‰å‡¦ç†
    - è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆåˆ†å‰²
    - ãƒãƒƒãƒç”Ÿæˆ
    """
    
    def __init__(self, data_dir="drowsiness_training_data"):
        """
        åˆæœŸåŒ–
        
        Args:
            data_dir (str): ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.data_dir = data_dir
        self.sessions_dir = os.path.join(data_dir, 'sessions')
        self.sequences_dir = os.path.join(data_dir, 'sequences')
        
        # ãƒ‡ãƒ¼ã‚¿
        self.all_sequences = []
        self.all_labels = []
        self.session_info = []
        
        # åˆ†å‰²å¾Œã®ãƒ‡ãƒ¼ã‚¿
        self.train_sequences = None
        self.train_labels = None
        self.val_sequences = None
        self.val_labels = None
        self.test_sequences = None
        self.test_labels = None
        
        # æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.scaler = None
        self.normalization_params = {
            'mean': None,
            'std': None,
            'is_fitted': False
        }
        
        print("=" * 70)
        print("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–")
        print("=" * 70)
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.data_dir}")
    
    def load_all_data(self, verbose=True) -> bool:
        """
        å…¨ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            verbose (bool): è©³ç´°è¡¨ç¤º
            
        Returns:
            bool: æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            sequence_files = glob.glob(os.path.join(self.sequences_dir, "*_sequences.npz"))
            
            if len(sequence_files) == 0:
                print("âš ï¸ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            self.all_sequences = []
            self.all_labels = []
            self.session_info = []
            
            if verbose:
                print(f"\nğŸ“‚ {len(sequence_files)} å€‹ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            
            for seq_file in sequence_files:
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                data = np.load(seq_file)
                sequences = data['sequences']
                labels = data['labels']
                session_name = str(data['session_name'])
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±èª­ã¿è¾¼ã¿
                info_file = os.path.join(
                    self.sessions_dir,
                    f"{session_name}_info.json"
                )
                
                if os.path.exists(info_file):
                    with open(info_file, 'r') as f:
                        session_info = json.load(f)
                    self.session_info.append(session_info)
                
                # ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                self.all_sequences.append(sequences)
                self.all_labels.append(labels)
                
                if verbose:
                    label_name = 'normal' if labels[0] == 0 else 'drowsy'
                    print(f"  âœ“ {session_name}: {len(sequences)} sequences ({label_name})")
            
            # çµ±åˆ
            self.all_sequences = np.vstack(self.all_sequences)
            self.all_labels = np.concatenate(self.all_labels)
            
            if verbose:
                print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
                print(f"   ç·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(self.all_sequences)}")
                print(f"   æ­£å¸¸: {np.sum(self.all_labels == 0)}")
                print(f"   çœ æ°—: {np.sum(self.all_labels == 1)}")
                print(f"   ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å½¢çŠ¶: {self.all_sequences.shape}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def split_data(self, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,
                   random_state=42, stratify=True, verbose=True):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆã«åˆ†å‰²
        
        Args:
            train_ratio (float): è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
            val_ratio (float): æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
            test_ratio (float): ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
            random_state (int): ä¹±æ•°ã‚·ãƒ¼ãƒ‰
            stratify (bool): å±¤åŒ–æŠ½å‡ºã‚’è¡Œã†ã‹
            verbose (bool): è©³ç´°è¡¨ç¤º
        """
        if len(self.all_sequences) == 0:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        # å‰²åˆã®ç¢ºèª
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "å‰²åˆã®åˆè¨ˆã¯1.0ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
        
        # å±¤åŒ–æŠ½å‡ºç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        stratify_param = self.all_labels if stratify else None
        
        # è¨“ç·´ + (æ¤œè¨¼ + ãƒ†ã‚¹ãƒˆ) ã«åˆ†å‰²
        train_val_ratio = val_ratio / (val_ratio + test_ratio)
        
        self.train_sequences, temp_sequences, self.train_labels, temp_labels = train_test_split(
            self.all_sequences,
            self.all_labels,
            test_size=(val_ratio + test_ratio),
            random_state=random_state,
            stratify=stratify_param
        )
        
        # (æ¤œè¨¼ + ãƒ†ã‚¹ãƒˆ) ã‚’ æ¤œè¨¼ã¨ãƒ†ã‚¹ãƒˆã«åˆ†å‰²
        stratify_param_temp = temp_labels if stratify else None
        
        self.val_sequences, self.test_sequences, self.val_labels, self.test_labels = train_test_split(
            temp_sequences,
            temp_labels,
            test_size=(test_ratio / (val_ratio + test_ratio)),
            random_state=random_state,
            stratify=stratify_param_temp
        )
        
        if verbose:
            print("\n" + "=" * 70)
            print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†")
            print("=" * 70)
            print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(self.train_sequences)} ({train_ratio*100:.1f}%)")
            print(f"  æ­£å¸¸: {np.sum(self.train_labels == 0)}, çœ æ°—: {np.sum(self.train_labels == 1)}")
            print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(self.val_sequences)} ({val_ratio*100:.1f}%)")
            print(f"  æ­£å¸¸: {np.sum(self.val_labels == 0)}, çœ æ°—: {np.sum(self.val_labels == 1)}")
            print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(self.test_sequences)} ({test_ratio*100:.1f}%)")
            print(f"  æ­£å¸¸: {np.sum(self.test_labels == 0)}, çœ æ°—: {np.sum(self.test_labels == 1)}")
            print("=" * 70)
    
    def normalize_data(self, method='zscore', verbose=True):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
        
        Args:
            method (str): æ­£è¦åŒ–æ‰‹æ³• ('zscore' or 'minmax')
            verbose (bool): è©³ç´°è¡¨ç¤º
        """
        if self.train_sequences is None:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒåˆ†å‰²ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—
        # shape: (n_samples, sequence_length, features) -> (n_samples * sequence_length, features)
        train_reshaped = self.train_sequences.reshape(-1, self.train_sequences.shape[-1])
        
        if method == 'zscore':
            # å¹³å‡0ã€æ¨™æº–åå·®1ã«æ­£è¦åŒ–
            mean = np.mean(train_reshaped, axis=0)
            std = np.std(train_reshaped, axis=0)
            std[std == 0] = 1.0  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
            
            self.normalization_params['mean'] = mean
            self.normalization_params['std'] = std
            self.normalization_params['is_fitted'] = True
            
            # é©ç”¨
            self.train_sequences = self._apply_zscore_normalization(self.train_sequences, mean, std)
            self.val_sequences = self._apply_zscore_normalization(self.val_sequences, mean, std)
            self.test_sequences = self._apply_zscore_normalization(self.test_sequences, mean, std)
            
            if verbose:
                print("\nâœ… Z-scoreæ­£è¦åŒ–å®Œäº†")
                print(f"   å¹³å‡: {mean}")
                print(f"   æ¨™æº–åå·®: {std}")
        
        elif method == 'minmax':
            # 0-1ã«æ­£è¦åŒ–
            min_val = np.min(train_reshaped, axis=0)
            max_val = np.max(train_reshaped, axis=0)
            range_val = max_val - min_val
            range_val[range_val == 0] = 1.0
            
            self.normalization_params['min'] = min_val
            self.normalization_params['max'] = max_val
            self.normalization_params['range'] = range_val
            self.normalization_params['is_fitted'] = True
            
            # é©ç”¨
            self.train_sequences = self._apply_minmax_normalization(self.train_sequences, min_val, range_val)
            self.val_sequences = self._apply_minmax_normalization(self.val_sequences, min_val, range_val)
            self.test_sequences = self._apply_minmax_normalization(self.test_sequences, min_val, range_val)
            
            if verbose:
                print("\nâœ… Min-Maxæ­£è¦åŒ–å®Œäº†")
        
        else:
            print(f"âŒ æœªçŸ¥ã®æ­£è¦åŒ–æ‰‹æ³•: {method}")
    
    def _apply_zscore_normalization(self, data, mean, std):
        """Z-scoreæ­£è¦åŒ–ã‚’é©ç”¨"""
        original_shape = data.shape
        data_reshaped = data.reshape(-1, data.shape[-1])
        normalized = (data_reshaped - mean) / std
        return normalized.reshape(original_shape)
    
    def _apply_minmax_normalization(self, data, min_val, range_val):
        """Min-Maxæ­£è¦åŒ–ã‚’é©ç”¨"""
        original_shape = data.shape
        data_reshaped = data.reshape(-1, data.shape[-1])
        normalized = (data_reshaped - min_val) / range_val
        return normalized.reshape(original_shape)
    
    def get_train_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return self.train_sequences, self.train_labels
    
    def get_val_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return self.val_sequences, self.val_labels
    
    def get_test_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return self.test_sequences, self.test_labels
    
    def save_normalization_params(self, filepath: str):
        """
        æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
        
        Args:
            filepath (str): ä¿å­˜å…ˆãƒ‘ã‚¹
        """
        if not self.normalization_params['is_fitted']:
            print("âš ï¸ æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæœªè¨­å®šã§ã™")
            return
        
        # NumPyé…åˆ—ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
        params_to_save = {}
        for key, value in self.normalization_params.items():
            if isinstance(value, np.ndarray):
                params_to_save[key] = value.tolist()
            else:
                params_to_save[key] = value
        
        with open(filepath, 'w') as f:
            json.dump(params_to_save, f, indent=2)
        
        print(f"âœ… æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜: {filepath}")
    
    def load_normalization_params(self, filepath: str):
        """
        æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            filepath (str): èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        try:
            with open(filepath, 'r') as f:
                params = json.load(f)
            
            # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
            for key, value in params.items():
                if isinstance(value, list):
                    self.normalization_params[key] = np.array(value, dtype=np.float32)
                else:
                    self.normalization_params[key] = value
            
            print(f"âœ… æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿: {filepath}")
        except Exception as e:
            print(f"âŒ æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_statistics(self) -> Dict:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Returns:
            Dict: çµ±è¨ˆæƒ…å ±
        """
        stats = {
            'total_sequences': len(self.all_sequences) if len(self.all_sequences) > 0 else 0,
            'total_sessions': len(self.session_info)
        }
        
        if len(self.all_sequences) > 0:
            stats['normal_count'] = int(np.sum(self.all_labels == 0))
            stats['drowsy_count'] = int(np.sum(self.all_labels == 1))
            stats['sequence_shape'] = self.all_sequences.shape
            stats['class_balance'] = {
                'normal': stats['normal_count'] / stats['total_sequences'],
                'drowsy': stats['drowsy_count'] / stats['total_sequences']
            }
        
        if self.train_sequences is not None:
            stats['train_count'] = len(self.train_sequences)
            stats['val_count'] = len(self.val_sequences)
            stats['test_count'] = len(self.test_sequences)
        
        return stats
    
    def print_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        stats = self.get_statistics()
        
        print("\n" + "=" * 70)
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ")
        print("=" * 70)
        print(f"ç·ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {stats['total_sessions']}")
        print(f"ç·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {stats['total_sequences']}")
        
        if 'normal_count' in stats:
            print(f"  æ­£å¸¸: {stats['normal_count']} ({stats['class_balance']['normal']*100:.1f}%)")
            print(f"  çœ æ°—: {stats['drowsy_count']} ({stats['class_balance']['drowsy']*100:.1f}%)")
            print(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å½¢çŠ¶: {stats['sequence_shape']}")
        
        if 'train_count' in stats:
            print(f"\nåˆ†å‰²å¾Œ:")
            print(f"  è¨“ç·´: {stats['train_count']}")
            print(f"  æ¤œè¨¼: {stats['val_count']}")
            print(f"  ãƒ†ã‚¹ãƒˆ: {stats['test_count']}")
        
        print("=" * 70)
    
    def export_dataset(self, output_path: str):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        
        Args:
            output_path (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        if self.train_sequences is None:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒåˆ†å‰²ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        np.savez(
            output_path,
            train_sequences=self.train_sequences,
            train_labels=self.train_labels,
            val_sequences=self.val_sequences,
            val_labels=self.val_labels,
            test_sequences=self.test_sequences,
            test_labels=self.test_labels,
            normalization_params=self.normalization_params
        )
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: {output_path}")
    
    def load_dataset(self, input_path: str):
        """
        ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
        
        Args:
            input_path (str): å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        try:
            data = np.load(input_path, allow_pickle=True)
            
            self.train_sequences = data['train_sequences']
            self.train_labels = data['train_labels']
            self.val_sequences = data['val_sequences']
            self.val_labels = data['val_labels']
            self.test_sequences = data['test_sequences']
            self.test_labels = data['test_labels']
            
            if 'normalization_params' in data:
                self.normalization_params = data['normalization_params'].item()
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿: {input_path}")
            self.print_statistics()
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    print("=" * 70)
    print("ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ä½œæˆ
    manager = DrowsinessDataManager()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ï¼‰
    print("\nãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã¾ã™...")
    success = manager.load_all_data(verbose=True)
    
    if success:
        # çµ±è¨ˆè¡¨ç¤º
        manager.print_statistics()
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        print("\nãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¾ã™...")
        manager.split_data(train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)
        
        # æ­£è¦åŒ–
        print("\nãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ã—ã¾ã™...")
        manager.normalize_data(method='zscore')
        
        # çµ±è¨ˆè¡¨ç¤º
        manager.print_statistics()
        
        # æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¿å­˜
        manager.save_normalization_params('normalization_params.json')
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        manager.export_dataset('drowsiness_dataset.npz')
        
        print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
    else:
        print("\nâš ï¸ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        print("   drowsiness_data_collector.py ã§ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦ãã ã•ã„")
    
    print("=" * 70)
